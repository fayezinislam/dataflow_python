{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1305f-2157-4daf-a21e-dbed80149159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74825abd-c5a2-4bce-bfa7-f919c109d364",
   "metadata": {},
   "source": [
    "# Running a Pub/Sub to Pub/Sub Dataflow Pipeline with Data Redaction\n",
    "\n",
    "This notebook guides you through setting up and running a streaming Dataflow\n",
    "pipeline that reads JSON messages from a Pub/Sub topic, removes\n",
    "specified fields (redaction), adds a processing timestamp, and writes the\n",
    "processed messages to an output Pub/Sub topic. Messages that fail parsing\n",
    "are routed to a dead-letter topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875c28a5-e2ae-4936-a101-2c1f6c8e9ddc",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "---\n",
    "### Check project permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c59bf8-9856-4173-bef0-fa4823eb8f0e",
   "metadata": {},
   "source": [
    "Before you begin your work on Google Cloud, you need to ensure that your project has the correct permissions within Identity and Access Management (IAM).\n",
    "\n",
    "    1. In the Google Cloud console, on the Navigation menu (Navigation menu icon), select IAM & Admin > IAM.\n",
    "\n",
    "    2. Confirm that the default compute Service Account {project-number}-compute@developer.gserviceaccount.com is present and has the editor role assigned. The account prefix is the project number, which you can find on Navigation menu > Cloud Overview > Dashboard.\n",
    "\n",
    "If the account is not present in IAM or does not have the editor role, follow the steps below to assign the required role.\n",
    "\n",
    "    1. In the Google Cloud console, on the Navigation menu, click Cloud Overview > Dashboard.\n",
    "    2. Copy the project number (e.g. 729328892908).\n",
    "    3. On the Navigation menu, select IAM & Admin > IAM.\n",
    "    4. At the top of the roles table, below View by Principals, click Grant Access.\n",
    "    5. For New principals, type:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bed19fbc-dbf2-4f4c-8e5e-166201e53949",
   "metadata": {},
   "source": [
    "  {project-number}-compute@developer.gserviceaccount.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f0b278-c6e8-4f84-bdd4-b8eb3e687399",
   "metadata": {},
   "source": [
    "    6. Replace {project-number} with your project number.\n",
    "    7. For Role, select Project (or Basic) > Editor.\n",
    "    8. Click Save."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de2e59e-4a7c-436b-be23-9dda9ca91823",
   "metadata": {},
   "source": [
    "### Jupyter notebook-based development environment setup\n",
    "For this lab, you will be running all commands in a terminal from your notebook.\n",
    "\n",
    "    1. In the Google Cloud Console, on the Navigation Menu, click Vertex AI > Workbench.\n",
    "    2. Click Enable Notebooks API.\n",
    "    3. On the Workbench page, select USER-MANAGED NOTEBOOKS and click CREATE NEW.\n",
    "    4. In the New instance dialog box that appears, set the region to region and zone to zone.\n",
    "    5. For Environment, select Apache Beam.\n",
    "    6. Click CREATE at the bottom of the dialog vox.\n",
    "        Note: The environment may take 3 - 5 minutes to be fully provisioned. Please wait until the step is complete.\n",
    "        Note: Click Enable Notebook API to enable the notebook api.\n",
    "    7. Once the environment is ready, click the OPEN JUPYTERLAB link next to your Notebook name. This will open up your environment in a new tab in your browser to run the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3676658-adee-4173-be8b-ad2761926baf",
   "metadata": {},
   "source": [
    "### Install Necessary Libraries\n",
    "First, ensure you have the required Python libraries installed.\n",
    "Create a virtual environment for your work in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f8bc3d-fc00-4074-974f-91341fb684ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo apt-get update && sudo apt-get install -y python3-venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d096d-0ce3-4073-a7c3-c789691ba0d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m venv df-env\n",
    "!source df-env/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d9b47-0258-4352-8566-6faccd9199a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade pip\n",
    "!pip install --quiet apache-beam[gcp] google-cloud-pubsub google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25742260-8aba-4810-b65b-e2179c60f285",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define GCP Configuration Variables\n",
    "Set your Google Cloud Project ID, Region, and desired names for\n",
    "the GCS Bucket and Pub/Sub topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebd6c9d-3e05-4e05-b9e6-8ba639173dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GCP_PROJECT_ID = \"\" # @param {type:\"string\"} Insert your Project ID here\n",
    "GCP_REGION = \"us-central1\" # @param {type:\"string\"} Specify the region for Dataflow and Pub/Sub\n",
    "BUCKET_NAME_SUFFIX = \"df-pipeline\" # @param {type:\"string\"} Suffix for unique bucket name\n",
    "INPUT_TOPIC_NAME = \"cdc-input-messages\" # @param {type:\"string\"} Name for the input Pub/Sub topic\n",
    "OUTPUT_TOPIC_NAME = \"cdc-output-messages\" # @param {type:\"string\"} Name for the output Pub/Sub topic\n",
    "DEAD_LETTER_TOPIC_NAME = \"cdc-dlq-messages\" # @param {type:\"string\"} Name for the dead-letter Pub/Sub topic\n",
    "FIELDS_TO_REMOVE = \"ssn\" # @param {type:\"string\"} Comma-separated fields to remove (e.g., \"ssn,email\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd67aaf-8cfe-4a53-89b6-7da4ebeb1d6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Set derived variables ---\n",
    "if not GCP_PROJECT_ID:\n",
    "    print(\"Please set the GCP_PROJECT_ID variable above.\")\n",
    "    # Attempt to get project ID from gcloud config if running locally\n",
    "    import subprocess\n",
    "    try:\n",
    "        GCP_PROJECT_ID = subprocess.check_output(['gcloud', 'config', 'get-value', 'project'], text=True).strip()\n",
    "        print(f\"Inferred GCP_PROJECT_ID: {GCP_PROJECT_ID}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not automatically determine Project ID. Please set it manually. Error: {e}\")\n",
    "        raise ValueError(\"GCP_PROJECT_ID is not set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fe11aa-6960-427c-8a10-d544fa1f817e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GCS_BUCKET = f\"gs://{GCP_PROJECT_ID}-{BUCKET_NAME_SUFFIX}\"\n",
    "PIPELINE_FOLDER = f\"{GCS_BUCKET}/df-pubsub_streaming_data_redaction_pipeline\"\n",
    "INPUT_TOPIC_PATH = f\"projects/{GCP_PROJECT_ID}/topics/{INPUT_TOPIC_NAME}\"\n",
    "OUTPUT_TOPIC_PATH = f\"projects/{GCP_PROJECT_ID}/topics/{OUTPUT_TOPIC_NAME}\"\n",
    "DEAD_LETTER_TOPIC_PATH = f\"projects/{GCP_PROJECT_ID}/topics/{DEAD_LETTER_TOPIC_NAME}\"\n",
    "RUNNER = \"DataflowRunner\" # Use DataflowRunner to run on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea7643e-8177-4746-8751-6c515bffcd16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set environment variables for the pipeline script\n",
    "import os\n",
    "os.environ['PROJECT_ID'] = GCP_PROJECT_ID\n",
    "os.environ['REGION'] = GCP_REGION\n",
    "os.environ['BUCKET'] = GCS_BUCKET\n",
    "os.environ['PIPELINE_FOLDER'] = PIPELINE_FOLDER\n",
    "os.environ['RUNNER'] = RUNNER\n",
    "os.environ['INPUT_TOPIC'] = INPUT_TOPIC_PATH\n",
    "os.environ['OUTPUT_TOPIC'] = OUTPUT_TOPIC_PATH\n",
    "os.environ['DEAD_LETTER_TOPIC'] = DEAD_LETTER_TOPIC_PATH\n",
    "os.environ['FIELDS_TO_REMOVE'] = FIELDS_TO_REMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820b7e7d-43f7-4bfd-89c9-8edd99578e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Project ID:          {os.environ['PROJECT_ID']}\")\n",
    "print(f\"Region:              {os.environ['REGION']}\")\n",
    "print(f\"GCS Bucket:          {os.environ['BUCKET']}\")\n",
    "print(f\"Pipeline Folder:     {os.environ['PIPELINE_FOLDER']}\")\n",
    "print(f\"Input Topic:         {os.environ['INPUT_TOPIC']}\")\n",
    "print(f\"Output Topic:        {os.environ['OUTPUT_TOPIC']}\")\n",
    "print(f\"Dead Letter Topic:   {os.environ['DEAD_LETTER_TOPIC']}\")\n",
    "print(f\"Fields to Remove:    {os.environ['FIELDS_TO_REMOVE']}\")\n",
    "print(f\"Runner:              {os.environ['RUNNER']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7572c0-a377-42a1-9bfc-e7b99ffe5cf2",
   "metadata": {},
   "source": [
    "### Enable Necessary GCP APIs\n",
    "Ensure that the Dataflow, Pub/Sub, and Cloud Storage APIs are enabled for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb1fdb-806c-48b1-9ef6-7b2cc6b7fa1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Enabling required Google Cloud APIs...\")\n",
    "!gcloud services enable dataflow.googleapis.com --project=$GCP_PROJECT_ID\n",
    "!gcloud services enable pubsub.googleapis.com --project=$GCP_PROJECT_ID\n",
    "!gcloud services enable storage-component.googleapis.com --project=$GCP_PROJECT_ID\n",
    "!gcloud services enable cloudresourcemanager.googleapis.com --project=$GCP_PROJECT_ID # Often needed by Dataflow\n",
    "print(\"APIs enabled (or already were).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fea5296-9568-4dd6-a924-752088ef20f8",
   "metadata": {},
   "source": [
    "### Create Google Cloud Storage Bucket\n",
    "Dataflow requires a GCS bucket for staging code and storing temporary files.\n",
    "We'll create one if it doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c3cdf4-cc72-4517-ab80-747281a1144b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Attempting to create GCS Bucket: {GCS_BUCKET}\")\n",
    "!gsutil mb -p $GCP_PROJECT_ID -l $GCP_REGION {GCS_BUCKET} || true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4350402-496a-4869-a19b-64e9f538078b",
   "metadata": {},
   "source": [
    "### Create Pub/Sub Topics\n",
    "Create the input, output, and dead-letter topics required by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d18147c-bd1c-4f65-8331-0a43b7031e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Creating Pub/Sub topics in project {GCP_PROJECT_ID}...\")\n",
    "\n",
    "print(f\"Creating Input Topic: {INPUT_TOPIC_PATH}\")\n",
    "!gcloud pubsub topics create {INPUT_TOPIC_NAME} --project=$GCP_PROJECT_ID || echo \"Input topic already exists or failed.\"\n",
    "\n",
    "print(f\"Creating Output Topic: {OUTPUT_TOPIC_PATH}\")\n",
    "!gcloud pubsub topics create {OUTPUT_TOPIC_NAME} --project=$GCP_PROJECT_ID || echo \"Output topic already exists or failed.\"\n",
    "\n",
    "print(f\"Creating Dead Letter Topic: {DEAD_LETTER_TOPIC_PATH}\")\n",
    "!gcloud pubsub topics create {DEAD_LETTER_TOPIC_NAME} --project=$GCP_PROJECT_ID || echo \"Dead Letter topic already exists or failed.\"\n",
    "\n",
    "print(\"Pub/Sub topic creation process finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278c6de-f102-430d-a5cc-d6f3136b6e71",
   "metadata": {},
   "source": [
    "## Define the Dataflow Pipeline Code\n",
    "---\n",
    "Save the provided Python script for the Dataflow pipeline to a local file\n",
    "within this notebook's environment using the `%%writefile` magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa86655-2749-4c72-8747-525524a09cf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile pubsub_streaming_data_redaction_pipeline.py\n",
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import typing\n",
    "from datetime import datetime, timezone\n",
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import TaggedOutput\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.options.pipeline_options import WorkerOptions\n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "from apache_beam.io.gcp.pubsub import ReadFromPubSub, WriteToPubSub\n",
    "\n",
    "# Helper Functions and Classes\n",
    "\n",
    "class ParseJsonWithDLQ(beam.DoFn):\n",
    "    \"\"\"\n",
    "    Parses a JSON byte string into a Python dictionary.\n",
    "    Outputs successfully parsed messages to the main output tag.\n",
    "    Outputs messages that fail parsing to the 'dead_letter_data' tag.\n",
    "    \"\"\"\n",
    "    SUCCESS_TAG = 'parsed_data'\n",
    "    FAILURE_TAG = 'dead_letter_data'\n",
    "\n",
    "    def process(self, element: bytes):\n",
    "        try:\n",
    "            parsed_dict = json.loads(element)\n",
    "            yield TaggedOutput(self.SUCCESS_TAG, parsed_dict)\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(f\"Failed to decode JSON: {e}. Message: {element[:100]}...\", exc_info=False)\n",
    "            yield TaggedOutput(self.FAILURE_TAG, element)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An unexpected error occurred during parsing: {e}. Message: {element[:100]}...\", exc_info=True)\n",
    "            yield TaggedOutput(self.FAILURE_TAG, element)\n",
    "\n",
    "def add_processing_timestamp(element: dict) -> dict:\n",
    "    \"\"\"Adds processing timestamps to a dictionary.\"\"\"\n",
    "    now_utc = datetime.now(timezone.utc)\n",
    "    element['processingTimestamp'] = now_utc.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    return element\n",
    "\n",
    "def encode_to_json_bytes(element: dict) -> bytes:\n",
    "    \"\"\"Encodes a dictionary to a JSON formatted byte string.\"\"\"\n",
    "    return json.dumps(element).encode('utf-8')\n",
    "\n",
    "class RemoveFieldsFn(beam.DoFn):\n",
    "    \"\"\"Removes specified fields from a dictionary element.\"\"\"\n",
    "    def __init__(self, fields_to_remove_set):\n",
    "        if isinstance(fields_to_remove_set, str):\n",
    "             self.fields_to_remove = set(f.strip() for f in fields_to_remove_set.split(',') if f.strip())\n",
    "        elif isinstance(fields_to_remove_set, (list, set)):\n",
    "             self.fields_to_remove = set(fields_to_remove_set)\n",
    "        else:\n",
    "             self.fields_to_remove = set()\n",
    "        logging.info(f\"RemoveFieldsFn initialized. Fields to remove: {self.fields_to_remove}\")\n",
    "\n",
    "    def process(self, element: dict):\n",
    "        if not self.fields_to_remove:\n",
    "            yield element\n",
    "            return\n",
    "        element_copy = element.copy()\n",
    "        for field in self.fields_to_remove:\n",
    "            element_copy.pop(field, None) # Use pop with default None to avoid KeyError if field doesn't exist\n",
    "        yield element_copy\n",
    "\n",
    "# Dataflow pipeline run method\n",
    "def run():\n",
    "    # Command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Load from Json from Pub/Sub, process events, write successes and failures to Pub/Sub')\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--staging_location', required=True, help='Specify Cloud Storage bucket for staging')\n",
    "    parser.add_argument('--temp_location', required=True, help='Specify Cloud Storage bucket for temp')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner (e.g., DataflowRunner, DirectRunner)')\n",
    "    parser.add_argument('--input_topic', required=True, help='Input Pub/Sub Topic (full path: projects/<PROJECT>/topics/<TOPIC>)')\n",
    "    parser.add_argument('--output_topic', required=True, help='Main output Pub/Sub Topic for successfully processed events (full path: projects/<PROJECT>/topics/<TOPIC>)')\n",
    "    parser.add_argument('--dead_letter_topic', required=True, help='Output Pub/Sub Topic for messages that failed processing (full path: projects/<PROJECT>/topics/<TOPIC>)')\n",
    "    parser.add_argument('--fields_to_remove', type=str, default=None, help='Comma-separated list of fields to remove from the main output (e.g., \"user_id,ip\")')\n",
    "    parser.add_argument('--num_workers', type=int, default=None, help='Initial number of workers for the job')\n",
    "    parser.add_argument('--max_num_workers', type=int, default=None, help='Max number of workers for the job')\n",
    "\n",
    "    opts = parser.parse_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options\n",
    "    options = PipelineOptions(save_main_session=True, streaming=True) # Make sure streaming is True\n",
    "    google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "    google_cloud_options.project = opts.project\n",
    "    google_cloud_options.region = opts.region\n",
    "    google_cloud_options.staging_location = opts.staging_location\n",
    "    google_cloud_options.temp_location = opts.temp_location\n",
    "    google_cloud_options.job_name = '{0}{1}'.format('df-pubsub-streaming-redaction-pipeline-',time.time_ns())\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "    options.view_as(WorkerOptions).num_workers = opts.num_workers\n",
    "    options.view_as(WorkerOptions).max_num_workers = opts.max_num_workers\n",
    "\n",
    "    input_topic = opts.input_topic\n",
    "    output_topic = opts.output_topic\n",
    "    dead_letter_topic = opts.dead_letter_topic\n",
    "\n",
    "    # Process fields_to_remove Argument\n",
    "    fields_to_remove_set = set()\n",
    "    if opts.fields_to_remove:\n",
    "        fields_to_remove_set = set(f.strip() for f in opts.fields_to_remove.split(',') if f.strip())\n",
    "\n",
    "    # Create the pipeline\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "    # Read from Pub/Sub\n",
    "    raw_msgs = (p | 'ReadFromPubSub' >> ReadFromPubSub(topic=input_topic).with_output_types(bytes))\n",
    "\n",
    "    # Parse JSON with Dead Letter Queue logic\n",
    "    parse_results = (raw_msgs | 'ParseJsonWithDLQ' >> beam.ParDo(ParseJsonWithDLQ()).with_outputs(ParseJsonWithDLQ.SUCCESS_TAG, ParseJsonWithDLQ.FAILURE_TAG))\n",
    "\n",
    "    # Handle Dead Letter Output\n",
    "    dead_letter_msgs = parse_results[ParseJsonWithDLQ.FAILURE_TAG]\n",
    "    (dead_letter_msgs | 'WriteDeadLetterToPubSub' >> WriteToPubSub(topic=dead_letter_topic))\n",
    "\n",
    "    # Handle Successful Output\n",
    "    parsed_msgs = parse_results[ParseJsonWithDLQ.SUCCESS_TAG]\n",
    "\n",
    "    # Continue processing successfully parsed messages\n",
    "    processed_events = (parsed_msgs | \"AddProcessingTimestamp\" >> beam.Map(add_processing_timestamp))\n",
    "\n",
    "    # Conditionally add the RemoveFieldsFn step\n",
    "    if fields_to_remove_set:\n",
    "        logging.info(f\"Adding step to remove fields: {fields_to_remove_set}\")\n",
    "        processed_events = (processed_events | 'RemoveSpecifiedFields' >> beam.ParDo(RemoveFieldsFn(fields_to_remove_set)))\n",
    "    else:\n",
    "         logging.info(\"No fields specified for removal.\")\n",
    "\n",
    "    # Encode dictionary to JSON bytes and write successfully processed events to Pub/Sub\n",
    "    (processed_events # Use the final processed PCollection\n",
    "        | \"EncodeSuccessToJson\" >> beam.Map(encode_to_json_bytes) # Changed label for clarity\n",
    "        | 'WriteSuccessToPubSub' >> WriteToPubSub(topic=output_topic)\n",
    "     )\n",
    "    # End Handle Successful Output ---\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline...\")\n",
    "    # Using current time: Monday, April 7, 2025 at 3:26:02 PM EDT (Example Time)\n",
    "    logging.info(f\"Current time: {datetime.now().isoformat()}\") # Log actual current time\n",
    "\n",
    "    # Run the pipeline\n",
    "    # For streaming pipelines, this initiates the job. Use wait_until_finish()\n",
    "    # only if you are running a batch job or want the script to block until\n",
    "    # the streaming job fails or is manually cancelled. Typically for streaming,\n",
    "    # you launch it and monitor elsewhere (like the GCP console).\n",
    "    pipeline_result = p.run()\n",
    "    # For a long-running streaming job, you might not want to wait here.\n",
    "    # pipeline_result.wait_until_finish() # Comment out for typical streaming deployment\n",
    "\n",
    "    logging.info(f\"Pipeline job started. Check the Dataflow UI: https://console.cloud.google.com/dataflow?project={opts.project}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c4bd82-5a3e-433b-8417-1362ba2b9bfa",
   "metadata": {},
   "source": [
    "## Run the Dataflow Pipeline\n",
    "---\n",
    "Now, execute the Python script using the environment variables we set earlier.\n",
    "This command submits the pipeline job to the Dataflow service.\n",
    "**Note:** This starts a *streaming* Dataflow job, which will run continuously\n",
    "until you manually stop or cancel it from the GCP Console or via gcloud command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b304f653-23ae-4bc6-a391-6d79610eff1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Submitting the Dataflow job...\")\n",
    "!python3 pubsub_streaming_data_redaction_pipeline.py \\\n",
    " --project=${PROJECT_ID} \\\n",
    " --region=${REGION} \\\n",
    " --staging_location=${PIPELINE_FOLDER}/staging \\\n",
    " --temp_location=${PIPELINE_FOLDER}/temp \\\n",
    " --runner=${RUNNER} \\\n",
    " --input_topic=${INPUT_TOPIC} \\\n",
    " --output_topic=${OUTPUT_TOPIC} \\\n",
    " --dead_letter_topic=${DEAD_LETTER_TOPIC} \\\n",
    " --fields_to_remove=\"${FIELDS_TO_REMOVE}\" \\\n",
    " --num_workers=1 \\\n",
    " --max_num_workers=3\n",
    "\n",
    "print(\"\\nPipeline submission command executed.\")\n",
    "print(f\"Check the Dataflow UI for job status: https://console.cloud.google.com/dataflow?project={GCP_PROJECT_ID}\")\n",
    "print(\"It might take a few minutes for the job to start running.\")\n",
    "\n",
    "# Wait a bit for messages to potentially arrive and be processed\n",
    "import time\n",
    "print(\"\\nWaiting 5 mins for the job to start...\")\n",
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd647c8-c05e-49ea-b386-0cc0c07f8967",
   "metadata": {},
   "source": [
    "## Test the Pipeline\n",
    "---\n",
    "### Create subscriptions to test the pipeline results\n",
    "Create subscriptions to the topics to check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f191b1c-e914-4fd9-861b-a9a70b3ed975",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create temporary subscription names\n",
    "OUTPUT_SUB_NAME = f\"{OUTPUT_TOPIC_NAME}-sub-temp\"\n",
    "DLQ_SUB_NAME = f\"{DEAD_LETTER_TOPIC_NAME}-sub-temp\"\n",
    "OUTPUT_SUB_PATH = f\"projects/{GCP_PROJECT_ID}/subscriptions/{OUTPUT_SUB_NAME}\"\n",
    "DLQ_SUB_PATH = f\"projects/{GCP_PROJECT_ID}/subscriptions/{DLQ_SUB_NAME}\"\n",
    "\n",
    "# Create subscriptions (use || true to ignore errors if they already exist)\n",
    "print(\"\\nCreating temporary subscriptions...\")\n",
    "!gcloud pubsub subscriptions create {OUTPUT_SUB_NAME} --topic={OUTPUT_TOPIC_NAME} --project={GCP_PROJECT_ID} --ack-deadline=60 || true\n",
    "!gcloud pubsub subscriptions create {DLQ_SUB_NAME} --topic={DEAD_LETTER_TOPIC_NAME} --project={GCP_PROJECT_ID} --ack-deadline=60 || true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd19508e-3cfd-4a4b-9f9f-4507f6f81190",
   "metadata": {},
   "source": [
    "### Publish a Sample Message\n",
    "Publish a test message to the input Pub/Sub topic and check that it landed in the output topic with the transformations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d80aed-0f63-4129-a730-9a80914d1916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the sample message as a Python dictionary\n",
    "sample_message_dict = {\n",
    "    \"commitTimestamp\": \"2025-04-07T20:07:10.123456Z\",\n",
    "    \"transactionId\": \"txn-cust-insert-9f8e7d6c\",\n",
    "    \"isLastRecordInTransactionInPartition\": True,\n",
    "    \"tableName\": \"customers\",\n",
    "    \"id\": \"cust-9f8e7d6c-1a2b-3c4d-5e6f-7g8h9i0j\",\n",
    "    \"name\": \"Charles Xavier\",\n",
    "    \"address\": \"1407 Graymalkin Lane, North Salem, NY 10560\",\n",
    "    \"email\": \"prof.x@example.org\",\n",
    "    \"ssn\": \"755-46-5678\", # This field should be removed based on FIELDS_TO_REMOVE\n",
    "    \"birth_date\": \"1982-11-01\",\n",
    "    \"modType\": \"INSERT\"\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a JSON string for publishing\n",
    "import json\n",
    "sample_message_json = json.dumps(sample_message_dict)\n",
    "\n",
    "print(f\"Publishing sample message to: {INPUT_TOPIC_PATH}\")\n",
    "!gcloud pubsub topics publish {INPUT_TOPIC_NAME} --project={GCP_PROJECT_ID} --message='{sample_message_json}'\n",
    "\n",
    "# Wait a bit for messages to potentially arrive and be processed\n",
    "import time\n",
    "print(\"\\nWaiting 10 seconds for messages to be processed by the pipeline...\")\n",
    "time.sleep(10)\n",
    "\n",
    "# Pull from the output subscription\n",
    "print(f\"\\n --- Checking Output Topic ({OUTPUT_TOPIC_PATH}) ---\")\n",
    "print(f\"Pulling messages from subscription: {OUTPUT_SUB_PATH}\")\n",
    "!gcloud pubsub subscriptions pull {OUTPUT_SUB_PATH} --auto-ack --limit=5\n",
    "\n",
    "print(f\"\\nExpected output message should have the 'ssn' field removed and a 'processingTimestamp' added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b05181-5c84-40c7-969a-2ea58dfbae9b",
   "metadata": {},
   "source": [
    "### Publish an Malformed Message (optional)\n",
    "Publish an invalid message to the input Pub/Sub topic and check that it landed in the DLQ topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d4ab3f-a057-4c56-b3e7-c33e3a023217",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "malformed_message_json = \"<THIS IS NOT A VALID JSON MESSAGE>\"\n",
    "print(f\"Publishing malformed message to: {INPUT_TOPIC_PATH}\")\n",
    "!gcloud pubsub topics publish {INPUT_TOPIC_NAME} --project={GCP_PROJECT_ID} --message='{malformed_message_json}'\n",
    "\n",
    "# Wait a bit for messages to potentially arrive and be processed\n",
    "import time\n",
    "print(\"\\nWaiting 10 seconds for messages to be processed by the pipeline...\")\n",
    "time.sleep(10)\n",
    "\n",
    "# Pull from the dead-letter subscription\n",
    "print(f\"\\n --- Checking Dead-Letter Topic ({DEAD_LETTER_TOPIC_PATH}) ---\")\n",
    "print(f\"Pulling messages from subscription: {DLQ_SUB_PATH}\")\n",
    "!gcloud pubsub subscriptions pull {DLQ_SUB_PATH} --auto-ack --limit=5\n",
    "\n",
    "print(f\"\\nExpected dead-letter message should be the raw, unparseable string: '{malformed_message_json}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c562bf-b686-4634-a411-219cf2b594eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Monitoring\n",
    "---\n",
    "You can monitor the running Dataflow job, view logs, and see metrics in the Google Cloud Console:\n",
    "[https://console.cloud.google.com/dataflow](https://console.cloud.google.com/dataflow?project={GCP_PROJECT_ID})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb737f9-4d35-4a8f-845d-9221fb6b8f61",
   "metadata": {},
   "source": [
    "## Cleanup (Important!)\n",
    "---\n",
    "**Streaming Dataflow jobs run continuously and incur costs until stopped.**\n",
    "Follow these steps to clean up the resources created in this tutorial.\n",
    "\n",
    "### Stop the Dataflow Job\n",
    "Find your job ID in the Dataflow UI or using the `gcloud` command below.\n",
    "Then, cancel the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0502a-3948-47b9-b3c4-d60157ba7432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Listing active Dataflow jobs in region\", GCP_REGION, \"(might take a moment)...\")\n",
    "!gcloud dataflow jobs list --region={GCP_REGION} --project={GCP_PROJECT_ID} --status=active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee34f4c7-4802-4801-bcfa-eb4217e1edc4",
   "metadata": {},
   "source": [
    "**ACTION REQUIRED:** Copy the `JOB_ID` of the 'df-pubsub-streaming-redaction-pipeline-...' job\n",
    "from the output above and paste it below. Then run the cell to cancel the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039d674-f4d0-48a5-9efc-12e9cbcbb094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATAFLOW_JOB_ID = \"\" # @param {type:\"string\"} PASTE JOB ID HERE\n",
    "\n",
    "if DATAFLOW_JOB_ID:\n",
    "    print(f\"\\nAttempting to cancel Dataflow job {DATAFLOW_JOB_ID} in region {GCP_REGION}...\")\n",
    "    !gcloud dataflow jobs cancel {DATAFLOW_JOB_ID} --region={GCP_REGION} --project={GCP_PROJECT_ID}\n",
    "else:\n",
    "    print(\"\\nPlease paste the Dataflow JOB_ID in the cell above and run it again to cancel the job.\")\n",
    "    print(f\"Alternatively, cancel the job manually via the Cloud Console: https://console.cloud.google.com/dataflow?project={GCP_PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485a989b-ed3e-47c9-a4f9-9e9d8c4ee093",
   "metadata": {},
   "source": [
    "### Delete Pub/Sub Subscriptions and Topics\n",
    "Delete the temporary subscriptions and the topics created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b54c17e-c75e-4d94-91c9-4351f6990309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nDeleting temporary Pub/Sub subscriptions...\")\n",
    "!gcloud pubsub subscriptions delete {OUTPUT_SUB_NAME} --project={GCP_PROJECT_ID} || echo \"Failed to delete output subscription (might not exist).\"\n",
    "!gcloud pubsub subscriptions delete {DLQ_SUB_NAME} --project={GCP_PROJECT_ID} || echo \"Failed to delete DLQ subscription (might not exist).\"\n",
    "\n",
    "print(\"\\nDeleting Pub/Sub topics...\")\n",
    "!gcloud pubsub topics delete {INPUT_TOPIC_NAME} --project={GCP_PROJECT_ID} || echo \"Failed to delete input topic.\"\n",
    "!gcloud pubsub topics delete {OUTPUT_TOPIC_NAME} --project={GCP_PROJECT_ID} || echo \"Failed to delete output topic.\"\n",
    "!gcloud pubsub topics delete {DEAD_LETTER_TOPIC_NAME} --project={GCP_PROJECT_ID} || echo \"Failed to delete DLQ topic.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d1e79d-9409-4a33-9714-fd465a65d25c",
   "metadata": {},
   "source": [
    "### Delete the Cloud Storage Bucket\n",
    "Delete the GCS bucket used for staging and temp files.\n",
    "**Warning:** This will permanently delete all contents of the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6daea3-4739-4adb-b7b4-04e7a79bc041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"\\nDeleting GCS bucket: {GCS_BUCKET}\")\n",
    "print(\"This may take a moment if the bucket contains objects...\")\n",
    "# Use -r to remove recursively, -f to force deletion without confirmation prompts\n",
    "!gsutil rm -r -f {GCS_BUCKET} || echo \"Failed to delete GCS bucket (might already be deleted or permissions issue).\"\n",
    "\n",
    "print(\"\\n--- Cleanup Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
